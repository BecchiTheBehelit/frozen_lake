# -*- coding: utf-8 -*-
"""frozen_lake.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13aAU0a8W8PejHQ_nlXUyJFI3L9wb78UH

# MDP

## Toy Game

[Frozen Lake](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/)


!apt install xvfb -y
!pip install pyvirtualdisplay
!pip install piglet
"""

import random
import gym
import numpy as np
import matplotlib.pyplot as plt
from IPython import display as ipythondisplay

env = gym.make('FrozenLake-v1')
env.reset()

from pyvirtualdisplay import Display
display = Display(visible=0, size=(400, 300))
display.start()

"""## Define MDP"""

class MDP():
    def __init__(self, num_states, num_actions, dynamics_fn):
        self.num_states = num_states
        self.num_actions = num_actions
        # P[s][a] represents a list of possible transistions given state s and a.
        # each transistion is expected as a list/tuple:
        # [prob_next_state, next_state, reward, is_terminal]
        self.P = dynamics_fn
        # sanity checks
        self.__verify()

    def __verify(self):
        assert len(self.P) == self.num_states
        for s in self.P.keys():
             assert len(self.P[s]) == self.num_actions
        for s in self.P.keys():
            for a in self.P[s].keys():
                transitions = self.P[s][a]
                p_sum = sum([t[0] for t in transitions])
                assert p_sum <= 1 and p_sum > 0.99

mdp = MDP(env.observation_space.n, env.action_space.n, env.unwrapped.P)

print("Number of states ", mdp.num_states)
print("Number of actions ", mdp.num_actions)

sample_actions = {'Up':0, 'RIGHT':1, "DOWN": 2, "LEFT": 3}
sample_states = [0,11,15]
for s in sample_states:
    for a in sample_actions.keys():
        print(f"Transitions for state {s} and action {a} are\n ", mdp.P[s][sample_actions[a]])

"""## Implement Agent

### Test Agent
"""

class RandomDeterministicAgent():
    def __init__(self, mdp, discount_rate=1.0, theta=0.05):
        self.mdp = mdp
        self.value_fn = [0] * self.mdp.num_states
        self.theta = theta
        num_a = self.mdp.num_actions
        num_s = self.mdp.num_states

        # random deterministic policy
        self.policy = []
        for s in range(num_s):
            p_s = [0] * num_a
            random_action = random.randint(0, num_a-1)
            p_s[random_action] = 1
            self.policy.append(p_s)
        self.discount_rate = discount_rate

    def get_action(self, s):
        """
        Using self.policy, return an action for state s.
        Supports stochastic policies.
        """
        current_action_prob_dist = self.policy[s]
        action = np.random.choice(list(range(self.mdp.num_actions)), 1, p=current_action_prob_dist)
        return action[0]


class RandomAgent():
    def __init__(self, mdp, discount_rate=1.0, theta=0.05):
        self.mdp = mdp
        self.value_fn = [0] * self.mdp.num_states
        self.theta = theta
        num_a = self.mdp.num_actions
        num_s = self.mdp.num_states

        # initital: random  policy
        self.policy = []
        for s in range(num_s):
            p_s = np.array([1] * num_a)
            p_s = p_s / 4
            self.policy.append(p_s)
        self.discount_rate = discount_rate

    def get_action(self, s):
        """
        Using self.policy, return an action for state s.
        Supports stochastic policies.
        """
        current_action_prob_dist = self.policy[s]
        action = np.random.choice(list(range(self.mdp.num_actions)), 1, p=current_action_prob_dist)
        return action[0]

"""### Value agent"""

class ValueAgent():
    def __init__(self, mdp, discount_rate=1.0, theta=0.000001):
        self.mdp = mdp
        self.value_fn = [0] * self.mdp.num_states
        self.theta = theta
        num_a = self.mdp.num_actions
        num_s = self.mdp.num_states
        self.discount_rate = discount_rate

        # random policy
        self.policy = []
        for s in range(num_s):
            p_s = [0] * num_a
            random_action = random.randint(0, num_a-1)
            p_s[random_action] = 1
            self.policy.append(p_s)

    def get_action(self, s):
        """
        Using self.policy, return an action for state s.
        Supports stochastic policies.
        """
        current_action_prob_dist = self.policy[s]
        action = np.random.choice(list(range(self.mdp.num_actions)), 1, p=current_action_prob_dist)
        return action[0]


    def evaluate_policy_for_state(self, s, action=None):
        """
        Policy evaluation for state s; action is optional.
        If action is None, it estimates state value function, v(s) = q(s, pi(s)).
        If action is given, it estimates state-action value function q(s,a).
        V_s = \sum{pr_s * (r_s + self.discount_rate * self.value_fn[s])}
        """
        q_value = 0
        for pr_s, ns, r, _ in self.mdp.P[s][action]:
            q_value += pr_s * (r + self.discount_rate * self.value_fn[ns])
        return q_value


    def update_policy(self, debug=False):
        """
        Using the current value function, improve the existing policy.
        Returns a boolean indicating whether the policy has changed.
        """
        """
        1. for each state s
        2. for each action a'
        3. calcuate q(s, a') -> call evaluate_policy_for_state(s, a')
        4. get the best action a', if it is not equal to a, update current policy(note that it is deterministic policy)
        """
        policy_changed = False
        for s in range(self.mdp.num_states):
            best_action = None
            best_q_value = float('-inf')
            for a in range(self.mdp.num_actions):
                q_value = self.evaluate_policy_for_state(s, a)
                if q_value > best_q_value:
                    best_action = a
                    best_q_value = q_value
            if self.policy[s][best_action] != 1:
                policy_changed = True
                self.policy[s] = [0] * self.mdp.num_actions
                self.policy[s][best_action] = 1
        return policy_changed

    def value_iteration(self, debug=False):
        """
        Using policy evaluation for state and policy improvement as subroutines,
        Calculates optimal value function first and extracts optimal policy from it.
        """
        while True:
            delta = 0
            for s in range(self.mdp.num_states):
                v = self.value_fn[s]
                q_values = [self.evaluate_policy_for_state(s, a) for a in range(self.mdp.num_actions)]
                self.value_fn[s] = max(q_values)
                delta = max(delta, abs(v - self.value_fn[s]))
            if delta < self.theta:
                break
        self.update_policy()

"""### Policy Agent"""

class PolicyAgent():
    def __init__(self, mdp, discount_rate=1.0, theta=0.000001):
        self.mdp = mdp
        self.value_fn = [0] * self.mdp.num_states
        self.theta = theta
        num_a = self.mdp.num_actions
        num_s = self.mdp.num_states
        self.discount_rate = discount_rate


        # random deterministic policy
        self.policy = []
        for s in range(num_s):
            p_s = [0] * num_a
            random_action = random.randint(0, num_a-1)
            p_s[random_action] = 1
            self.policy.append(p_s)

    def get_action(self, s):
        """
        Using self.policy, return an action for state s.
        Supports stochastic policies.
        """
        current_action_prob_dist = self.policy[s]
        action = np.random.choice(list(range(self.mdp.num_actions)), 1, p=current_action_prob_dist)
        return action[0]


    def evaluate_policy_for_state(self, s, action=None):
        """
        Policy evaluation for state s; action is optional.
        Action is None, it estimates state value function, v(s) = q(s, pi(s)).
        V_s = \sum{pr_s * (r_s + self.discount_rate * self.value_fn[s])}
        """
        action = self.get_action(s)
        q_value = 0
        for pr_s, ns, r, _ in self.mdp.P[s][action]:
            q_value += pr_s * (r + self.discount_rate * self.value_fn[ns])
        return q_value

    def evaluate_policy(self):
        """
        Policy evaluation for all states.
        Uses self.theta to determine stopping distance.
        """
        """
        1. for each state
        2. calcuate v_s = v(s, pi(s))
        3. repeat util converage
        """

        while True:
            delta = 0
            for s in range(self.mdp.num_states):
                v = self.value_fn[s]
                self.value_fn[s] = self.evaluate_policy_for_state(s)
                delta = max(delta, abs(v - self.value_fn[s]))
            if delta < self.theta:
                break

    def update_policy(self, debug=False):
        """
        Using the current value function, improve the existing policy.
        Returns a boolean indicating whether the policy has changed.
        """
        policy_changed = False
        for s in range(self.mdp.num_states):
            current_action = self.get_action(s)
            best_action = None
            best_q_value = float('-inf')
            for a in range(self.mdp.num_actions):
                q_value = 0
                for pr_s, ns, r, _ in self.mdp.P[s][a]:
                    q_value += pr_s * (r + self.discount_rate * self.value_fn[ns])
                if q_value > best_q_value:
                    best_action = a
                    best_q_value = q_value
            if current_action != best_action:
                policy_changed = True
                self.policy[s] = [0] * self.mdp.num_actions
                self.policy[s][best_action] = 1
        return policy_changed

    def policy_iteration(self, debug=False):
        """
        Using policy evaluation and policy iteration as subroutines,
        Calculates optimal value function and optimal policy.
        """
        """
        1. evaluate_policy
        2. update_policy
        3. repeat util stable
        """
        while True:
            self.evaluate_policy()
            if not self.update_policy():
                break

"""## Play Game"""

def run_experiment(env, agent, num_runs=1):
    for _ in range(num_runs):
        tot_reward = [0]
        env.reset()
        observation = 0
        done = False
        reward_per_run = 0
        img = plt.imshow(env.render('rgb_array')) # only call this once
        while not done:
            action = agent.get_action(observation)
            print(observation, action)
            observation, reward, done, info = env.step(action)
            reward_per_run += reward

            screen = env.render(mode='rgb_array')
            plt.imshow(screen)
            ipythondisplay.clear_output(wait=True)
            ipythondisplay.display(plt.gcf())


        env.close()
        tot_reward.append(reward_per_run + tot_reward[-1])
    return tot_reward

"""## Testing test angent"""

run_experiment(env, RandomDeterministicAgent(mdp))

run_experiment(env, RandomAgent(mdp))

"""## Testing Value Agent"""

agent1 = ValueAgent(mdp)
agent1.value_iteration()
run_experiment(env, agent1)

"""## Testing Policy Agent"""

agent2 = PolicyAgent(mdp)
agent2.policy_iteration()
run_experiment(env, agent2)